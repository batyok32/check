import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity, manhattan_
distances, euclidean_distances
from sklearn.feature_extraction.text import TfidfVectorizer
import re
from gensim import models
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.style
%matplotlib inline
from gensim.models import FastText as ft
from IPython.display import Image
import os

Content_df = pd.read_csv("Rec_sys_content.csv")

Content_df.shape

# Total Null Values in Data
Content_df.isnull().sum(axis = 0)


#Importing Word2Vec
word2vecModel = models.KeyedVectors.load_word2vec_format('GoogleNews-­
vectors-­negative300.bin.gz', binary=True)

#Importing FastText
fasttext_model=ft.load_fasttext_format("cc.en.300.bin.gz")

#Import Glove
glove_df = pd.read_csv('glove.6B.300d.txt', sep=" ",
                       quoting=3, header=None, index_col=0)
glove_model = {key: value.values for key, value in glove_df.T.items()}

## Combining Product and Description
Content_df['Description'] = Content_df['Product Name'] + ' ' + Content_df['Description']

# Dropping Duplicates and keeping first record
unique_df = Content_df.drop_duplicates(subset=['Description'], keep='first')

# Converting String to Lower Case
unique_df['desc_lowered'] = unique_df['Description'].apply(lambda x: x.lower())

# Remove Stop special Characters
unique_df['desc_lowered'] = unique_df['desc_lowered'].apply(lambda x: re.sub(r'[^\w\s]', '', x))

# Coverting Description to List
desc_list = list(unique_df['desc_lowered'])
unique_df= unique_df.reset_index(drop=True)


#Importing Count Vectorizer
cnt_vec = CountVectorizer(stop_words='english')

# Importing IFIDF
tfidf_vec = TfidfVectorizer(stop_words='english', analyzer='word', ngram_range=(1,3))


def find_euclidean_distances(sim_matrix, index, n=10):
    # Getting Score and Index
    result = list(enumerate(sim_matrix[index]))
    # Sorting the Score and taking top 10 products
    sorted_result = sorted(result,key=lambda x:x[1],reverse=False)[1:10+1]
    # Mapping index with data
    similar_products =  [{'value': unique_df.iloc[x[0]]['Product Name'], 'score' : round(x[1], 2)} for x in sorted_result]
    return similar_products


#Cosine similarity
def find_similarity(cosine_sim_matrix, index, n=10):
    # calculate cosine similarity between each vectors
    result = list(enumerate(cosine_sim_matrix[index]))
    # Sorting the Score
    sorted_result = sorted(result,key=lambda x:x[1],reverse=True)[1:n+1]
    similar_products =  [{'value': unique_df.iloc[x[0]]['Product Name'], 'score' : round(x[1], 2)} for x in sorted_result]
    return similar_products

#Manhattan distance
def find_manhattan_distance(sim_matrix, index, n=10):
    # Getting Score and Index
    result = list(enumerate(sim_matrix[index]))
    # Sorting the Score and taking top 10 products
    sorted_result = sorted(result,key=lambda x:x[1],reverse=False)[1:10+1]
    # Mapping index with data
    similar_products =  [{'value': unique_df.iloc[x[0]]['Product Name'], 'score' : round(x[1], 2)} for x in sorted_result]
    return similar_products

def get_recommendation_cv(product_id, df, similarity, n=10):
    row = df.loc[df['Product Name'] == product_id]
    index = list(row.index)[0]
    description = row['desc_lowered'].loc[index]
    #Create vector using Count Vectorizer
    count_vector = cnt_vec.fit_transform(desc_list)
    if similarity == "cosine":
        sim_matrix = cosine_similarity(count_vector)
        products = find_similarity(sim_matrix , index)
    elif similarity == "manhattan":
        sim_matrix = manhattan_distances(count_vector)
        products = find_manhattan_distance(sim_matrix , index)
    else:
        sim_matrix = euclidean_distances(count_vector)
        products = find_euclidean_distances(sim_matrix , index)
    return products


product_id = 'Vickerman 14" Finial Drop Christmas Ornaments, Pack of 2'

# Cosine Similarity
#get_recommendation_cv(product_id, unique_df, similarity = "cosine", n=10)

#Manhattan Similarity
#get_recommendation_cv(product_id, unique_df, similarity ="manhattan", n=10)

#Euclidean Similarity
#get_recommendation_cv(product_id, unique_df, similarity ="euclidean", n=10)

# Comparing similarity to get the top matches using TF-IDF
def get_recommendation_tfidf(product_id, df, similarity, n=10):
    row = df.loc[df['Product Name'] == product_id]
    index = list(row.index)[0]
    description = row['desc_lowered'].loc[index]
    #Create vector using tfidf
    tfidf_matrix = tfidf_vec.fit_transform(desc_list)
    if similarity == "cosine":
        sim_matrix = cosine_similarity(tfidf_matrix)
        products = find_similarity(sim_matrix , index)
    elif similarity == "manhattan":
        sim_matrix = manhattan_distances(tfidf_matrix)
        products = find_manhattan_distance(sim_matrix , index)
    else:
        sim_matrix = euclidean_distances(tfidf_matrix)
        products = find_euclidean_distances(sim_matrix , index)
    return products

# Cosine Similarity
#get_recommendation_tfidf(product_id, unique_df, similarity ="cosine", n=10)

#Manhattan Similarity
#get_recommendation_tfidf(product_id, unique_df, similarity ="manhattan", n=10)

#Euclidean Similarity
#get_recommendation_tfidf(product_id, unique_df, similarity ="euclidean", n=10)



def get_recommendation_word2vec(product_id, df, similarity, n=10):
    row = df.loc[df['Product Name'] == product_id]
    input_index = list(row.index)[0]
    description = row['desc_lowered'].loc[input_index]
    #create vectors for each desc using word2vec
    vector_matrix = np.empty((len(desc_list), 300))
    for index, each_sentence in enumerate(desc_list):
        sentence_vector = np.zeros((300,))
        count  = 0
        for each_word in each_sentence.split():
            try:
                sentence_vector += word2vecModel[each_word]
                count += 1
            except:
                continue
            
            vector_matrix[index] = sentence_vector
    if similarity == "cosine":
        sim_matrix = cosine_similarity(vector_matrix)
        products = find_similarity(sim_matrix , input_index)
    elif similarity == "manhattan":
        sim_matrix = manhattan_distances(vector_matrix)
        products = find_manhattan_distance(sim_matrix , input_index)
    else:
        sim_matrix = euclidean_distances(vector_matrix)
        products = find_euclidean_distances(sim_matrix , input_index)
    return products

#Manhattan Similarity
get_recommendation_word2vec(product_id, unique_df, similarity = "manhattan", n=10)

# Cosine Similarity
#get_recommendation_word2vec(product_id, unique_df, similarity ="cosine", n=10)

#Euclidean Similarity
#get_recommendation_word2vec(product_id, unique_df, similarity ="euclidean", n=10)


def get_recommendation_fasttext(product_id, df, similarity, n=10):
    row = df.loc[df['Product Name'] == product_id]
    input_index = list(row.index)[0]
    description = row['desc_lowered'].loc[input_index]
    #create vectors for each description using fasttext
    vector_matrix = np.empty((len(desc_list), 300))
    for index, each_sentence in enumerate(desc_list):
        sentence_vector = np.zeros((300,))
        count  = 0
        for each_word in each_sentence.split():
            try:
                sentence_vector += fasttext_model.wv[each_word]
                count += 1
            except:
                continue
            vector_matrix[index] = sentence_vector
    if similarity == "cosine":
        sim_matrix = cosine_similarity(vector_matrix)
        products = find_similarity(sim_matrix , input_index)
    elif similarity == "manhattan":
        sim_matrix = manhattan_distances(vector_matrix)
        products = find_manhattan_distance(sim_matrix , input_index)
    else:
        sim_matrix = euclidean_distances(vector_matrix)
        products = find_euclidean_distances(sim_matrix , input_index)
    return products

# Cosine Similarity
get_recommendation_fasttext(product_id, unique_df, similarity ="cosine", n=10)

#Manhattan Similarity
get_recommendation_fasttext(product_id, unique_df, similarity ="manhattan", n=10)

#Euclidean Similarity
get_recommendation_fasttext(product_id, unique_df, similarity ="euclidean", n=10)


#  Comparing similarity to get the top matches using GloVe pretrained model
def get_recommendation_glove(product_id, df, similarity, n=10):
    row = df.loc[df['Product Name'] == product_id]
    input_index = list(row.index)[0]
    description = row['desc_lowered'].loc[input_index]
    #using glove embeddings to create vectors
    vector_matrix = np.empty((len(desc_list), 300))
    for index, each_sentence in enumerate(desc_list):
        sentence_vector = np.zeros((300,))
        count  = 0
        for each_word in each_sentence.split():
            try:
                sentence_vector += glove_model[each_word]
                count += 1
            except:
                continue
        vector_matrix[index] = sentence_vector
    if similarity == "cosine":
        sim_matrix = cosine_similarity(vector_matrix)
        products = find_similarity(sim_matrix , input_index)
    elif similarity == "manhattan":
        sim_matrix = manhattan_distances(vector_matrix)
        products = find_manhattan_distance(sim_matrix , input_index)
    else:
        sim_matrix = euclidean_distances(vector_matrix)
        products = find_euclidean_distances(sim_matrix , input_index)
    return products

#Euclidean Similarity
get_recommendation_glove(product_id, unique_df, similarity ="euclidean", n=10)

# Cosine Similarity
get_recommendation_glove(product_id, unique_df, similarity = "cosine", n=10)

#Manhattan Similarity
get_recommendation_glove(product_id, unique_df, similarity ="manhattan", n=10)